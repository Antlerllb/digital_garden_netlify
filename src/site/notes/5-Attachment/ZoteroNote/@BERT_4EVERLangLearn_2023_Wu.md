---
{"dg-publish":true,"permalink":"/5-Attachment/ZoteroNote/@BERT_4EVERLangLearn_2023_Wu/","title":"BERT_4EVER at LangLearn: Language Development Assessment Model based on Sequential Information Attention Mechanism"}
---

# 1 BERT_4EVER at LangLearn: Language Development Assessment Model based on Sequential Information Attention Mechanism
## 1.1 Link
- Item: [item](zotero://select/library/items/PW7SLKZ5)
- File: [BERT_4EVERLangLearn_2023_Wu.pdf](zotero://open-pdf/library/items/SIDIPHTX)
## 1.2 Abstract
In recent years, investigations into language acquisition have greatly benefited from the utilization of natural language processing technologies, particularly in analyzing extensive corpora consisting of authentic texts produced by learners across the realms of first and second language acquisition. A crucial task in this domain involves the assessment of language learners’ language ability development. The “Language Learning Development” task featured in EVALITA 2023 [1] marks a significant milestone as the inaugural shared task focused on automated language development assessment, which entails predicting the relative order of two essays written by the same student. We introduce a novel attention mechanism, namely sequential information attention mechanism, with the primary objective of exploiting information interaction between sequence texts. Experimental results on the COWS dataset show the effectiveness of our proposed sequential information attention mechanism, showcasing its substantial impact on model performance during the final evaluation phase.