---
{"dg-publish":true,"permalink":"/1-Project/语言习得/Exp部分的实验/"}
---

# 1 结果+解释

## 1.1 效果差
[[5-Attachment/ZoteroNote/@SelfICLZeroShot_2023_Chen\|@SelfICLZeroShot_2023_Chen]]
*InstructGPT > GPT-3.5*
**Interestingly**, we find GPT-3.5 has a **slightly inferior** performance comparing to InstructGPT.
**We hypothesize** this is because GPT-3.5 has a **lower controllabil  ity**, thus, it is more prone to generate unintended content.
**For instance**, the model might not follow the formatting instructions presented in the prompts (see Figure 8).
**In addition**, the generated pseudo inputs are more likely to be **invalid** and **could not accurately** represent the underlying tasks.
# 2 图表观察+解释
## 2.1 两句
[[5-Attachment/ZoteroNote/@SelfICLZeroShot_2023_Chen\|@SelfICLZeroShot_2023_Chen]]
**As observed**, batch inference produces pseudo inputs that is most similar to using real inputs.
**This is somewhat intuitive as** batch inference has access to more example instances.

[[5-Attachment/ZoteroNote/@SelfICLZeroShot_2023_Chen\|@SelfICLZeroShot_2023_Chen]]
**Interestingly, looking at results in Figure 5 we find** using pseudo-inputs generated by prompting with diversity hints (the 3-shot bar) and batch inference achieve essentially the same final accuracy, **although** it exhibits the lower similarity.
**This may suggest** over diversifying demo-inputs **have little impact** on empirical performance. For prompting without diversity hints, **it demonstrates** the highest similarity to test input and lower final accuracy, **which could be explained by** copying effect.

[[5-Attachment/ZoteroNote/@SelfICLZeroShot_2023_Chen\|@SelfICLZeroShot_2023_Chen]]
**As observed**, the 3-shot is the top performing setup.
**Note that although** inferior to 3-shot, 1-shot **still exhibits** a notable gain over 0-shot, **indicating the empirical effectiveness of** SELF-ICL.
## 2.2 一句
Although the performance drop using random labels **may indicate the possibility that** some instances encounter the copying effect,
