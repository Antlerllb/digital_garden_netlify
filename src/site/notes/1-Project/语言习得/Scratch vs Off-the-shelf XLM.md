---
{"dg-publish":true,"permalink":"/1-Project/语言习得/Scratch vs Off-the-shelf XLM/"}
---

[[5-Attachment/ZoteroNote/@SecondLanguage_2023_Oba\|@SecondLanguage_2023_Oba]]
XLM: Cross-lingual Language Model
# 1 off-the-shelf XLM
- is trained across dozens of languages (Conneau and Lample, 2019; Conneau et al., 2020)
- super-multilingual setting: unrealistic
	- humans hardly face dozens of languages in a multilingual acquisition scenario
# 2 Scratch
- 18M
- tokenizer: byte pair encoding on the bilingual texts
- trained the model separately for each L1–L2 combination
