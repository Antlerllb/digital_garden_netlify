---
{"dg-publish":true,"permalink":"/1-Project/语言习得/ICL 简单介绍/"}
---

[[5-Attachment/ZoteroNote/@FiDICLFusioninDecoder_2023_Ye\|@FiDICLFusioninDecoder_2023_Ye]]
- By concatenating a few examples and prepending them before the test instance, the model can perform a new task readily (Brown et al., 2020)

[[5-Attachment/ZoteroNote/@FewshotIncontext_2023_Li\|@FewshotIncontext_2023_Li]]
- In-context learning with large language models (Brown et al., 2020a) has shown strong few-shot performance in many NLP tasks
- such as question answering (Cheng et al., 2022), information extraction (Dunn et al., 2022), and numerical reasoning (Lewkowycz et al., 2022).

[[5-Attachment/ZoteroNote/@GPTREIncontext_2023_Wan\|@GPTREIncontext_2023_Wan]]
- LLMs employ a new paradigm known as incontext learning (ICL) (Brown et al., 2020; Min et al., 2022a) 
- formulates an NLP task under the paradigm of language generation and makes predictions by learning from a few demonstrations.

[[5-Attachment/ZoteroNote/@SelfICLZeroShot_2023_Chen\|@SelfICLZeroShot_2023_Chen]]
- Large language models (LMs) have shown striking ability to adapt to new tasks at test time by prompting with a few input-output exemplars, i.e., demonstrations (Brown et al., 2020; Wei et al., 2022; Chowdhery et al., 2022; Wei et al., 2023).
- This ability is refereed to as in-context learning (ICL; Brown et al., 2020).