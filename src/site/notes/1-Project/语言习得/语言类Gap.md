---
{"dg-publish":true,"permalink":"/1-Project/语言习得/语言类Gap/"}
---

# 1 按linguistic phenomena分类
[[5-Attachment/ZoteroNote/@BLiMPBenchmark_2020_Warstadt\|@BLiMPBenchmark_2020_Warstadt]]
- 现状
	- We conclude that whereas models like GPT-2 appear to have significant linguistic knowledge, this knowledge is **concentrated in some specific domains of English grammar**.
- 我们的理解与方案
	- how the linguistic knowledge of state-of-the-art language models (LMs) varies across the linguistic phenomena of English.
	- We use BLiMP to **uncover several linguistic phenomena** where even state-of-the-art language models clearly **lack human-like knowledge**, and to bring into focus those areas of grammar that future studies evaluating LMs should investigate in greater depth.
# 2 大方向的结论
[[5-Attachment/ZoteroNote/@BLiMPBenchmark_2020_Warstadt\|@BLiMPBenchmark_2020_Warstadt]]
However, each of these studies uses a different set of metrics, and focuses on a small set of linguistic paradigms, severely limiting any **possible big-picture conclusions**.
# 3 还没整理的杂项
[[1-Project/语言习得/LI-Adger#1 Improves upon BLiMP and Zorro\|LI-Adger#1 Improves upon BLiMP and Zorro]]